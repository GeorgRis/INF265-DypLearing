{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Avoids warning from tokenizer when using num_workers > 0 in DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-only Transformer for Text Sentiment Classification\n",
    "\n",
    "## 1. Data\n",
    "\n",
    "### 1.1 Load the IMDB dataset\n",
    "\n",
    "We will use the IMDB dataset for sentiment classification. The dataset consists of 50,000 movie reviews, each labeled as positive or negative. We will use 25k reviews for training, 5k for validation, and 20k for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid pattern: '**' can only be an entire path component",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m VAL_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load IMDB dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Split test into test and validation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/datasets/load.py:1773\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1768\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1769\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1770\u001b[0m )\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1773\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1774\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1775\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1776\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1777\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1778\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1779\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m   1780\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1781\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1782\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1783\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m   1784\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1785\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs,\n\u001b[1;32m   1786\u001b[0m )\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/datasets/load.py:1502\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1501\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1502\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1503\u001b[0m     path,\n\u001b[1;32m   1504\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1505\u001b[0m     download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1506\u001b[0m     download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1507\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1508\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/datasets/load.py:1219\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1215\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1216\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1218\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1223\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/datasets/load.py:1203\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1189\u001b[0m             path,\n\u001b[1;32m   1190\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m             dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1194\u001b[0m         )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1197\u001b[0m             path,\n\u001b[1;32m   1198\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1199\u001b[0m             data_dir\u001b[38;5;241m=\u001b[39mdata_dir,\n\u001b[1;32m   1200\u001b[0m             data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m   1201\u001b[0m             download_config\u001b[38;5;241m=\u001b[39mdownload_config,\n\u001b[1;32m   1202\u001b[0m             download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[0;32m-> 1203\u001b[0m         )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e1:  \u001b[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/datasets/load.py:769\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetModule:\n\u001b[1;32m    760\u001b[0m     hfh_dataset_info \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    762\u001b[0m         revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[1;32m    763\u001b[0m         token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token,\n\u001b[1;32m    764\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    767\u001b[0m         sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m    768\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns_in_dataset_repository(hfh_dataset_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m    770\u001b[0m     )\n\u001b[1;32m    771\u001b[0m     data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_hf_repo(\n\u001b[1;32m    772\u001b[0m         patterns,\n\u001b[1;32m    773\u001b[0m         dataset_info\u001b[38;5;241m=\u001b[39mhfh_dataset_info,\n\u001b[1;32m    774\u001b[0m         base_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir,\n\u001b[1;32m    775\u001b[0m         allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    776\u001b[0m     )\n\u001b[1;32m    777\u001b[0m     split_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    778\u001b[0m         split: infer_module_for_data_files(data_files_list, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token)\n\u001b[1;32m    779\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m split, data_files_list \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    780\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/datasets/data_files.py:662\u001b[0m, in \u001b[0;36mget_data_patterns_in_dataset_repository\u001b[0;34m(dataset_info, base_path)\u001b[0m\n\u001b[1;32m    660\u001b[0m resolver \u001b[38;5;241m=\u001b[39m partial(_resolve_single_pattern_in_dataset_repository, dataset_info, base_path\u001b[38;5;241m=\u001b[39mbase_path)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_data_files_patterns(resolver)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EmptyDatasetError(\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset repository at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain any data files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/datasets/data_files.py:223\u001b[0m, in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m--> 223\u001b[0m         data_files \u001b[38;5;241m=\u001b[39m pattern_resolver(pattern)\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    225\u001b[0m             non_empty_splits\u001b[38;5;241m.\u001b[39mappend(split)\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/datasets/data_files.py:473\u001b[0m, in \u001b[0;36m_resolve_single_pattern_in_dataset_repository\u001b[0;34m(dataset_info, pattern, base_path, allowed_extensions)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 473\u001b[0m glob_iter \u001b[38;5;241m=\u001b[39m [PurePath(filepath) \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mglob(PurePath(pattern)\u001b[38;5;241m.\u001b[39mas_posix()) \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(filepath)]\n\u001b[1;32m    474\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    475\u001b[0m     filepath\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m glob_iter\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    484\u001b[0m ]  \u001b[38;5;66;03m# ignore .ipynb and __pycache__, but keep /../\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/fsspec/spec.py:611\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    609\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind(root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 611\u001b[0m pattern \u001b[38;5;241m=\u001b[39m glob_translate(path \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ends_with_sep \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    612\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n\u001b[1;32m    614\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    615\u001b[0m     p: info\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(allpaths\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m     )\n\u001b[1;32m    622\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/INF265/lib/python3.11/site-packages/fsspec/utils.py:731\u001b[0m, in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m part:\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid pattern: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only be an entire path component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m     )\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part:\n\u001b[1;32m    735\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(_translate(part, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_sep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, not_sep))\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "SEED = 420\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "# Load IMDB dataset\n",
    "dataset_train = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset_test = load_dataset(\"imdb\", split=\"test\")\n",
    "\n",
    "# Split test into test and validation\n",
    "dataset_test = dataset_test.train_test_split(test_size=1-VAL_SIZE, seed=SEED)\n",
    "dataset_test, dataset_val = dataset_test[\"test\"], dataset_test[\"train\"]\n",
    "\n",
    "print(f\"Train size: {len(dataset_train)}\")\n",
    "print(f\"Validation size: {len(dataset_val)}\")\n",
    "print(f\"Test size: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess the data\n",
    "\n",
    "We remove HTML tags, special characters, and convert the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    # Remove HTML tags\n",
    "    return re.sub(r'<[^>]*>', '', text)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Remove special characters except for ,.!? and space\n",
    "    return re.sub(r'[^a-zA-Z0-9.,!? ]', '', text)\n",
    "\n",
    "def to_lowercase(text):\n",
    "    # Convert text to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Apply all preprocessing steps to the text\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = to_lowercase(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_batch(examples):\n",
    "    # Apply preprocessing to all texts in the batch\n",
    "    examples[\"text\"] = [preprocess_text(text) for text in examples[\"text\"]]\n",
    "    return examples\n",
    "\n",
    "# Preprocess the dataset\n",
    "dataset_train = dataset_train.map(preprocess_batch, batched=True)\n",
    "dataset_val = dataset_val.map(preprocess_batch, batched=True)\n",
    "dataset_test = dataset_test.map(preprocess_batch, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization\n",
    "\n",
    "We use a simple word-level tokenizer to tokenize the text data. We use three special tokens: `[PAD]`, `[UNK]` and `[CLS]`. The `[PAD]` token will be used to pad the input sequences to the same length. The `[UNK]` token is used to represent out-of-vocabulary words (rare words). When classifying text, we will prepend the `[CLS]` token to the input sequence and use its output as the representation of the whole sequence.\n",
    "\n",
    "To reduce the vocabulary size, we only keep words appearing at least `MIN_FREQUENCY` times in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens (padding, unknown, classification)\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "CLS_TOKEN = \"[CLS]\"\n",
    "MIN_FREQUENCY = 10\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
    "tokenizer.pre_tokenizer = Whitespace() # Split on whitespace\n",
    "\n",
    "# Train tokenizer on the training set\n",
    "trainer = WordLevelTrainer(special_tokens=[PAD_TOKEN, UNK_TOKEN, CLS_TOKEN], min_frequency=MIN_FREQUENCY, vocab_size=VOCAB_SIZE)\n",
    "tokenizer.train_from_iterator(dataset_train[\"text\"], trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the tokenizer on a few examples to verify that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    example = dataset_train[idx]\n",
    "    original_text = example[\"text\"]\n",
    "    tokenized_text = tokenizer.encode(example[\"text\"])\n",
    "    decoded_text = tokenizer.decode(tokenized_text.ids, skip_special_tokens=False)\n",
    "\n",
    "    print(f\"Original text: {original_text}\")\n",
    "    print(f\"Tokenized text (tokens): {tokenized_text.tokens}\")\n",
    "    print(f\"Tokenized text (IDs): {tokenized_text.ids}\")\n",
    "    print(f\"Decoded text: {decoded_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the distribution of the sequence lengths in the training set. Later, we can use this to determine the maximum sequence length to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of token lengths\n",
    "token_lengths = [len(tokenizer.encode(text).ids) for text in dataset_train[\"text\"]]\n",
    "median_length = np.median(token_lengths)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(token_lengths, bins=50, color=\"black\")\n",
    "ax.set_xticks(np.arange(0, np.max(token_lengths) + 1, 100))\n",
    "ax.tick_params(axis=\"x\", labelrotation=45)\n",
    "ax.set_xlabel(\"Number of tokens in sequence\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.axvline(median_length, color=\"red\", linestyle=\"--\", label=f\"Median length: {median_length}\")\n",
    "ax.legend()\n",
    "fig.suptitle(\"Histogram of token lengths\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Dataset and Data Loaders\n",
    "\n",
    "To deal with sequences of different lengths, we implement a simple `IMDBDataset` class that pads the sequences to the same length using the `[PAD]` token, or truncates them if they exceed the maximum sequence length. We also prepend the `[CLS]` token to the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length, pad_idx, cls_idx):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "        self.cls_idx = cls_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO ✅\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        tokenized = self.tokenizer.encode(text).ids\n",
    "        tokenized = [self.cls_idx] + tokenized\n",
    "        \n",
    "        if len(tokenized) < self.max_length:\n",
    "            tokenized = tokenized + [self.pad_idx] * (self.max_length - len(tokenized))\n",
    "        else:\n",
    "            tokenized = tokenized[:self.max_length]\n",
    "            #konverterer til tensor i retunen \n",
    "        return torch.tensor(tokenized), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some constants:\n",
    "\n",
    "- `MAX_LENGTH`: the maximum sequence length\n",
    "- `PAD_ID`: the ID of the `[PAD]` token\n",
    "- `CLS_ID`: the ID of the `[CLS]` token\n",
    "- `BATCH_SIZE`: the batch size\n",
    "- `NUM_WORKERS`: the number of workers for data loading (set this to 0 if you are encountering issues with the DataLoader)\n",
    "\n",
    "We also create the training and testing datasets and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_LENGTH = 256\n",
    "PAD_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "CLS_ID = tokenizer.token_to_id(CLS_TOKEN)\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Dataset instances\n",
    "train_dataset = IMDBDataset(dataset_train[\"text\"], dataset_train[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
    "val_dataset = IMDBDataset(dataset_val[\"text\"], dataset_val[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
    "test_dataset = IMDBDataset(dataset_test[\"text\"], dataset_test[\"label\"], tokenizer, MAX_LENGTH, PAD_ID, CLS_ID)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, generator=torch.Generator().manual_seed(SEED))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training Loop\n",
    "\n",
    "We define a simple train function that takes `model` and trains it for `num_epochs` on `train_loader`. We give the `criterion` (loss function) and `optimizer` as arguments to the function. The train function also needs to know `pad_id`, the ID of the `[PAD]` token so we can generate the attention mask making sure the model does not attend to the padding tokens. After each epoch, we evaluate the model on `val_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(sequences, pad_id):\n",
    "    \"\"\"\n",
    "    Input shape of token sequences: (batch_size, seq_length)\n",
    "    Output shape: (batch_size, seq_length) boolean mask with True where the padding tokens are located\n",
    "    \"\"\"\n",
    "    # TODO ✅\n",
    "    # returnerer en boolsk maske samme form som seqence \n",
    "    # True er pad tolken \n",
    "    return sequences == pad_id\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, pad_id):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        correct, total = 0, 0\n",
    "        for sequences, labels in (pbar := tqdm(train_loader)):\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            attention_mask = create_mask(sequences, pad_id)\n",
    "            outputs = model(sequences, mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # Clip gradients for stability\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            correct += ((outputs > 0.5) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            pbar.set_description(f\"Epoch {epoch+1}, Train Loss: {np.mean(train_losses):.4f}, Train Acc.: {correct / total:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in tqdm(val_loader):\n",
    "                labels = labels.float()\n",
    "                attention_mask = create_mask(sequences, pad_id)\n",
    "                outputs = model(sequences, mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_losses.append(loss.item())\n",
    "                correct += ((outputs > 0.5) == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {np.mean(val_losses):.4f}, Val Acc.: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multi-Head Attention\n",
    "\n",
    "We implement the multi-head attention mechanism. The multi-head attention mechanism consists of `num_heads` independent attention mechanisms. We concatenate the outputs of the different heads and project them back to the model's dimension. We use the scaled dot-product attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        \"\"\"\n",
    "        Multihead attention module.\n",
    "        Args:\n",
    "            dim: Dimension of the input vectors\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout rate for attention scores (default: 0.1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert dim % num_heads == 0, f\"Dimension {dim} must be divisible by num_heads {num_heads}\"\n",
    "\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        # TODO: Linear transformations for query, key, and value\n",
    "        # ✅\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "\n",
    "        # TODO: Output linear transformation\n",
    "        # ✅\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask):\n",
    "        # TODO: Implement this method\n",
    "        # ✅\n",
    "        # linær transformation af query, key og value\n",
    "        batch_size, seq_length, _ = query.size()\n",
    "        \n",
    "        # Lineær transformasjon\n",
    "        Q = self.query(query)  # (batch_size, seq_length, dim)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "        \n",
    "        # endre form \n",
    "        # (batch_size, num_heads, seq_length, head_dim)\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Skalerte \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        # scores: (batch_size, num_heads, seq_length, seq_length)\n",
    "        \n",
    "        if key_padding_mask is not None:\n",
    "            # Utvid masken til å passe score-dimensjonen\n",
    "            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  \n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Softmax for å få attention-vektene\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        # attn_weights: (batch_size, num_heads, seq_length, seq_length)\n",
    "        attn_output = torch.matmul(attn_weights, V) \n",
    "        # attn_output: (batch_size, num_heads, seq_length, head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.dim)\n",
    "        \n",
    "        # Output-projeksjon\n",
    "        output = self.out_proj(attn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Block\n",
    "\n",
    "We implement the encoder block, which consists of our multi-head attention layer followed by a feedforward neural network. We also add residual connections and layer normalizations as specified in the project description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # TODO: Implement this method ✅\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Feedforward-nettverk \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * dim, dim)\n",
    "        )\n",
    "    def forward(self, x, mask=None):\n",
    "        # TODO: Implement this method ✅\n",
    "        # multihead attention med residual connection og layer norm\n",
    "        attn_out = self.attention(x, x, x, key_padding_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # feedforward med residual connection og layer norm\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Model Specification\n",
    "\n",
    "We specify our classifier model, which consists of an embedding layer, followed by `num_layers` encoder blocks. We use a linear layer to project the output of the last encoder block and apply sigmoid activation to get the final output.\n",
    "\n",
    "We also define the positional encoding, which is added to the input embeddings to give the model information about the position of the tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding module: adds positional information to the input embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, max_len):\n",
    "        super().__init__()\n",
    "        # TODO: Implement this method ✅\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-np.log(10000.0) / embed_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, embed_size)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Implement this method (add positional encodings to the input embeddings x) ✅\n",
    "        seq_length = x.size(1)\n",
    "        x = x + self.pe[:, :seq_length, :]\n",
    "        return x\n",
    "\n",
    "class SentimentTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embedding_dim, num_heads, num_layers, pad_idx, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, max_len)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([EncoderBlock(embedding_dim, num_heads, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, 1)  # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        x = self.pos_encoder(x)  # Add positional encodings\n",
    "\n",
    "        for encoder in self.encoder:\n",
    "            x = encoder(x, mask=mask)\n",
    "\n",
    "        x = x[:, 0, :]  # Take the first token's embedding (CLS token equivalent)\n",
    "\n",
    "        return self.sigmoid(self.fc(x)).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Model\n",
    "\n",
    "We can now train the model. It is recommended that you use the parameters provided in the cell below as they should work pretty good without requiring too much computational power.\n",
    "\n",
    "Train the model for (at least) 3 epochs. Each epoch should take around 7 to 12 minutes on a modern CPU. If you are training on a laptop, make sure it is plugged in. Moreover, closing all other applications can also help. Take a well-deserved break while the model trains.\n",
    "\n",
    "If you struggle with extremely long training times, you can do one or more of the following to speed up training:\n",
    "\n",
    "- Increase `MIN_FREQUENCY` and re-train the tokenizer to reduce the vocabulary size.\n",
    "- Use a subset of the training data.\n",
    "- Reduce the number of epochs.\n",
    "- Reduce the maximum sequence length.\n",
    "- Experiment with `NUM_WORKERS` in the data loaders (optimal value is system-dependent).\n",
    "- If you have a GPU, modify the training function above to use it.\n",
    "\n",
    "Expect to observe a training accuracy above `0.60` halfway through the first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-3\n",
    "EMBEDDING_DIM = 96\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "model = SentimentTransformer(vocab_size=VOCAB_SIZE, \n",
    "                             max_len=MAX_LENGTH,\n",
    "                             embedding_dim=EMBEDDING_DIM, \n",
    "                             num_heads=NUM_HEADS, \n",
    "                             num_layers=NUM_LAYERS, \n",
    "                             pad_idx=PAD_ID)\n",
    "\n",
    "criterion = nn.BCELoss() # Model output should have sigmoid applied!\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {n_params}\")\n",
    "\n",
    "train_model(model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            pad_id=PAD_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluating the Model on Unseen Data\n",
    "\n",
    "We evaluate the model on the test set and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, pad_id):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in tqdm(test_loader):\n",
    "            attention_mask = create_mask(sequences, pad_id)\n",
    "            outputs = model(sequences, mask=attention_mask)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate_model(model, test_loader, PAD_ID)\n",
    "print(f\"Accuracy (test): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Testing the Model on Custom Examples\n",
    "\n",
    "For fun, we test the model on some custom examples from IMDb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify a single review\n",
    "def classify_review(review, model, tokenizer, pad_idx, cls_idx, max_length):\n",
    "    # TODO: Implement this function ✅\n",
    "    # Remember to set the model to evaluation mode, preprocess the review, tokenize it with [CLS] token prepended, pad/truncate and create a mask before passing it to the model\n",
    "    model.eval()\n",
    "    # preprocess the review \n",
    "    preprocess = lambda x: tokenizer(x, padding=True, truncation=True, max_length=max_length, return_tensors='pt').input_ids\n",
    "    review = preprocess(review)\n",
    "    \n",
    "    # Tokeniser\n",
    "    tokenized = tokenizer.encode(preprocess).ids\n",
    "    tokenized = [cls_idx] + tokenized\n",
    "    \n",
    "    # Truncate eller pad sekvensen til max_length\n",
    "    if len(tokenized) > max_length:\n",
    "        tokenized = tokenized[:max_length]\n",
    "    else:\n",
    "        tokenized = tokenized + [pad_idx] * (max_length - len(tokenized))\n",
    "    \n",
    "    # Konverter til tensor og legg til batch-dimensjon\n",
    "    input_tensor = torch.tensor(tokenized, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    # Lag padding-masken\n",
    "    mask = create_mask(input_tensor, pad_idx)\n",
    "    \n",
    "    # Kjør modellen\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, mask=mask)\n",
    "    \n",
    "    # Siden output er en sannsynlighet (etter sigmoid), bruk terskel på 0.5\n",
    "    p = output.item()\n",
    "    sentiment = \"Positiv\" if p > 0.5 else \"Negativ\"\n",
    "    \n",
    "    \n",
    "    return sentiment, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Classify some reviews chosen by you\n",
    "class SentimentTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embedding_dim, num_heads, num_layers, pad_idx, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, max_len)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncoderBlock(embedding_dim, num_heads, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)          # (batch_size, seq_length, embedding_dim)\n",
    "        x = self.pos_encoder(x)\n",
    "        for encoder_block in self.encoder:\n",
    "            x = encoder_block(x, mask=mask)\n",
    "        # Bruk den første tokenens embedding ([CLS]) for klassifisering\n",
    "        x = x[:, 0, :]  \n",
    "        return self.sigmoid(self.fc(x)).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
